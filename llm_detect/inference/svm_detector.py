
import re
import logging
from typing import List

import pandas as pd
import numpy as np
import skops.io
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split, GridSearchCV


logger = logging.getLogger(__name__)

TOKEN_RE = re.compile(r'[a-zA-Z0-9]+')


def load_data(data_path, test_size=0.20, random_state=42, label_key='label'):
    data = pd.read_csv(data_path, sep=None)
    data = data[data['text'].str.len().between(500, 5000)]
    X, y = data['text'], data[label_key]

    if test_size > 0:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, shuffle=True, random_state=random_state, stratify=y)
        return X_train, X_test, y_train, y_test
    return X, y


def train_model(X_train, X_test, y_train, y_test, output_path):
    pipe = make_pipeline(TfidfVectorizer(), SGDClassifier())

    # hyper-parameter tuning
    grid =  [{
        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)],
        'sgdclassifier__loss': ['hinge', 'log_loss', 'perceptron']}]
    gs = GridSearchCV(pipe, grid, cv=5, scoring='f1_macro', n_jobs=-1, refit=True, verbose=1)
    gs.fit(X_train, y_train)
    logger.info("- Best training CV f1_macro: {:.3f}".format(gs.best_score_))
    logger.info("- Best params: {}".format(gs.best_params_))

    logger.info("- Calibrating model on test set")
    # SGDClassifier doens't output probabilities, so calibrate it on the test split
    clf = gs.best_estimator_
    # refit with best parameters
    clf.fit(X_train, y_train)
    clf = CalibratedClassifierCV(clf, cv="prefit").fit(X_test, y_test)

    logger.info("- Serializing model to: {}".format(output_path))
    skops.io.dump(clf, output_path)


def load_model(path):
    skops.io.get_untrusted_types(file=path)
    return skops.io.load(
            path,
            trusted=['sklearn.calibration._CalibratedClassifier',
                    'sklearn.calibration._SigmoidCalibration'])


def get_coefficients(clf, top_k=0, normalize=False):
    """
    Get token-level class scores using the support vector weights.
    Zeroth scores for tokens that are past the `top_k` of either class.
    """
    coefficients = clf.estimator.named_steps.sgdclassifier.coef_[0]
    feature_names = clf.estimator.named_steps.tfidfvectorizer.get_feature_names_out()

    sort = np.argsort(coefficients)
    coefficients, feature_names = coefficients[sort], feature_names[sort]

    if normalize: # squeeze to (-1, 1)
        neg, = np.where(np.array(coefficients) < 0)
        coefficients_neg = coefficients[neg] / -np.min(coefficients[neg])
        pos, = np.where(np.array(coefficients) >= 0)
        coefficients_pos = coefficients[pos] / np.max(coefficients[pos])
        coefficients = np.concatenate([coefficients_neg, coefficients_pos])

    # zero-th out of top-k
    if top_k > 0:
        coefficients[top_k:-top_k] = 0
    return dict(zip(feature_names, coefficients))


def nonlinear(x, a, b):
    return x / (a*(b-x)+b)


class SVMDetector:
    def __init__(self, model, top_k=0, cue_percentile_cutoff=0.75):
        """
        SVM-based pre-trained detector that outputs confidence score as well as per-token class weights.
        
        - `top_k`: if greater than 0 it only takes into account the scores of words that are in the `top_k`
            of each class, zeroing out all the other scores.
        - `normalize`: if True, the scores are normalize to the (0, 1) range for the positive class and 
            (-1, 0) for the negative class.
        - `cue_percentile_cutoff`: a float in the (0, 1) range, defining the percentile cutoff for cue words.
        """
        self.model = model
        self.coefficients = get_coefficients(model, top_k=top_k)
        if cue_percentile_cutoff < 0 or cue_percentile_cutoff > 1:
            raise ValueError("`cue_percentile_cutoff` must be in range [0, 1]")
        coefs = np.array(list(self.coefficients.values()))    
        self.pos_cutoff = np.percentile(coefs[coefs>=0], [cue_percentile_cutoff * 100])
        self.neg_cutoff = np.percentile(-coefs[coefs<0], [cue_percentile_cutoff * 100])
        # for token-level scores
        self.norm_coefficients = get_coefficients(model, top_k=top_k, normalize=True)

    @classmethod
    def from_file(cls, path, **kwargs):
        model = load_model(path)
        return cls(model, **kwargs)

    def score(self, text):
        """
        Score the probability that the input text is generated by an AI system.
        """
        _, score = self.model.predict_proba([text])[0]
        return score
    
    def score_tokens(self, text):
        """
        Return token-level scores. Scores are always normalized.
        """
        return self._score_tokens(text, use_cue_words=False)

    def score_sentences(self, sentences: List[str]):
        """
        Return sentence-level scores using cue-words. Token-level scores are converted into
        minus one if a token is deemed to be a cue word of the negative class and +1 if the
        token is deemed to be a cue word of the positive class. The definition of a cue word
        is based on `cue_percentile_cutoff`.
        """
        output = []

        for sent in sentences:
            tokens, scores = zip(*self._score_tokens(sent, use_cue_words=True))
            is_cue = np.array(list(filter(None, scores)))
            # just proportion of words in each class
            prop = is_cue.sum() / len(is_cue)
            # apply nonlinear transform
            if prop < 0:
                prop = -nonlinear(-prop, 0.8, 1)
            else:
                prop = nonlinear(prop, 0.8, 1)
            output.append((''.join(tokens), prop))

        return output

    def _score_tokens(self, text, use_cue_words):
        """
        Identify class-relevant tokens in the input text using the SVM coefficients
        """
        output = []
        last = 0
        for match in TOKEN_RE.finditer(text):
            start, end = match.span()
            # append interim
            if last != start != 0:
                output.append((text[last:start], None))
            # find feature
            word = text[start:end]
            # get score
            score = None
            if word in self.coefficients:
                # signal cue words instead of returning the score
                if use_cue_words:
                    score = self.coefficients[word]
                    if score < 0 and -score >= self.neg_cutoff:
                        score = -1
                    elif score > 0 and score >= self.pos_cutoff:
                        score = 1
                    else:
                        score = None
                else:
                    # use normalized scores (-1, 1)
                    score = self.norm_coefficients[word]
            output.append((text[start:end], score))
            last = end
        # trailing text
        if last != len(text):
            output.append((text[last:], None))

        return output


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('input_path')
    parser.add_argument('output_path')
    parser.add_argument('--label_key', default='label')
    args = parser.parse_args()

    from llm_detect.settings import setup_logger
    setup_logger()

    X_train, X_test, y_train, y_test = load_data(args.input_path, label_key=args.label_key)
    train_model(X_train, X_test, y_train, y_test, args.output_path)

import re
import pandas as pd
import numpy as np
import skops.io

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split, GridSearchCV

from settings import settings
import utils


def load_data(data_path='data/data.csv', test_size=0.20, random_state=42):
    data = pd.read_csv(data_path)
    data = data[data['text'].str.len().between(500, 5000)]
    X, y = data['text'], data['label']

    if test_size > 0:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, shuffle=True, random_state=random_state, stratify=y)
        return X_train, X_test, y_train, y_test
    return X, y

    
def train_model(X_train, X_test, y_train, y_test, output_path=settings.SVM_BASELINE_PATH):
    pipe = make_pipeline(TfidfVectorizer(), SGDClassifier())

    # hyper-parameter tuning
    grid =  [{
        'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)],
        'sgdclassifier__loss': ['hinge', 'log_loss', 'perceptron']}]
    gs = GridSearchCV(pipe, grid, cv=5, scoring='f1_macro', n_jobs=-1, refit=True, verbose=1)
    gs.fit(X_train, y_train)
    print("- Best training CV f1_macro: {:.3f}".format(gs.best_score_))
    print("- Best params: ", gs.best_params_)

    print("- Calibrating model on test set")
    # SGDClassifier doens't output probabilities, so calibrate it on the test split
    clf = gs.best_estimator_
    # refit with best parameters
    clf.fit(X_train, y_train)
    clf = CalibratedClassifierCV(clf, cv="prefit").fit(X_test, y_test)

    print("- Serializing model")
    skops.io.dump(clf, output_path)


def load_model(path='svm_baseline.skops'):
    skops.io.get_untrusted_types(file=path)
    return skops.io.load(
        'svm_baseline.skops',
        trusted=['sklearn.calibration._CalibratedClassifier',
                 'sklearn.calibration._SigmoidCalibration'])


def get_coefficients(clf, top_k=0, normalize=False):
    """
    Get token-level class scores using the support vector weights.
    Zeroth scores for tokens that are past the `top_k` of either class.
    """
    coefficients = clf.estimator.named_steps.sgdclassifier.coef_[0]
    feature_names = clf.estimator.named_steps.tfidfvectorizer.get_feature_names_out()

    sort = np.argsort(coefficients)
    coefficients, feature_names = coefficients[sort], feature_names[sort]

    if normalize: # squeeze to (-1, 1)
        neg, = np.where(np.array(coefficients) < 0)
        coefficients_neg = coefficients[neg] / -np.min(coefficients[neg])
        pos, = np.where(np.array(coefficients) >= 0)
        coefficients_pos = coefficients[pos] / np.max(coefficients[pos])
        coefficients = np.concatenate([coefficients_neg, coefficients_pos])

    # zero-th out of top-k
    if top_k > 0:
        coefficients[top_k:-top_k] = 0
    return dict(zip(feature_names, coefficients))


class SVMDetector:
    def __init__(self, path, top_k=0, normalize=False, use_cue_words=False, cue_percentile_cutoff=0):
        """
        SVM-based pre-trained detector that outputs confidence score as well as per-token class weights.
        
        - `top_k`: if greater than 0 it only takes into account the scores of words that are in the `top_k`
            of each class, zeroing out all the other scores.
        - `normalize`: if True, the scores are normalize to the (0, 1) range for the positive class and 
            (-1, 0) for the negative class.
        - `use_cue_words`: if True, scores are converted into -1 if a token is deemed to be a cue word of
            the negative class and +1 if a token is deemed to be a cue word of the positive class.
            The definition of a cue word is based on `cue_percentile_cutoff`. If True, `top_k` should be 0
            since otherwise the interaction may result in unexpected outputs.
        - `cue_percentile_cutoff`: a float in the (0, 1) range, defining the percentile cutoff for cue words.
        """
        self.model = load_model(path)
        self.coefficients = get_coefficients(self.model, top_k=top_k, normalize=normalize)
        self.use_cue_words = use_cue_words
        if use_cue_words > 0:
            if cue_percentile_cutoff < 0 or cue_percentile_cutoff > 1:
                raise ValueError("`cue_percentile_cutoff` must be in range [0, 1]")
            coefs = np.array(list(self.coefficients.values()))    
            self.pos_cutoff = np.percentile(coefs[coefs>=0], [cue_percentile_cutoff * 100])
            self.neg_cutoff = np.percentile(-coefs[coefs<0], [cue_percentile_cutoff * 100])

    def score(self, text, return_token_scores=True, split_sentences=True):
        """
        Score the probability that the input text is generated by an AI system.

        - return_token_scores: if True it also returns scores per token
            The format of the token scores depends on `top_k`, `normalize` `use_cue_words`.
        - split_sentences: if True, the input text is first split into sentences
        """
        _, score = self.model.predict_proba([text])[0]
        if return_token_scores:
            if split_sentences:
                token_scores = self._score_tokens_by_sent(text)
            else:
                token_scores = self._score_tokens(text)
            return score, token_scores    
        return score

    def _score_tokens(self, text):
        """
        Identify class-relevant tokens in the input text using the model-assigned class scores.
        """
        TOKEN_RE = re.compile(r'[a-zA-Z0-9]+')
        output = []
        last = 0
        for match in TOKEN_RE.finditer(text):
            start, end = match.span()
            # append interim
            if last != start != 0:
                output.append((text[last:start], None))
            # find feature
            word = text[start:end]
            # get score
            score = None
            if word in self.coefficients:
                score = self.coefficients[word]
                # signal cue words instead of returning the score
                if self.use_cue_words:
                    if score < 0 and -score >= self.neg_cutoff:
                        score = -1
                    elif score > 0 and score >= self.pos_cutoff:
                        score = 1
                    else:
                        score = None
            output.append((text[start:end], score))
            last = end
        # trailing text
        if last != len(text):
            output.append((text[last:], None))
        
        return output
    
    def _score_tokens_by_sent(self, text):
        sents = []
        for sent in utils.segment_text(text):
            sents.append(self._score_tokens(sent))
        return sents

